{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "\n",
    "# !pip install llama-index-embeddings-huggingface\n",
    "# !pip install google-generativeai>=0.3.0 llama-index-embeddings-gemini\n",
    "# !pip install llama-index-llms-cohere\n",
    "# !pip install llama-index-embeddings-cohere\n",
    "\n",
    "# !pip install llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    StorageContext, \n",
    "    ServiceContext, \n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\custom_rag_llamaindex\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(f'Current Working Directory: {cwd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reader: <llama_index.core.readers.file.base.SimpleDirectoryReader object at 0x000001BCD1767970>\n",
      "Documents: [Document(id_='a91546ca-ecbe-4fba-9d20-6688c23f60df', embedding=None, metadata={'page_label': '1', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Lifecycle of  \\nmachine learning models\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ab354848-f3b7-4858-8b2c-b88208de9b8c', embedding=None, metadata={'page_label': '2', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='      1\\n2/Introduction \\n Steps of building machine learning models \\n2/ Machine learning is an area that enterprises \\nare increasingly investing in or identifying as a potential area of growth. There are many reasons enterprises invest in machine learning, from being able to leverage data to find insights about their customers to making processes more efficient. In this book, we break down how machine learning models are built into six steps: data access and collection, data preparation and exploration, model build and train, model evaluation, model deployment, and model monitoring. \\nBuilding a machine learning model is an iterative \\nprocess. Many of the steps needed to build a machine learning model are reiterated and modified until data scientists are satisfied with the model performance. This process requires a great deal of data exploration, visualization and experimentation as each step must be explored, modified and audited independently. Data access and collection \\nData preparation and exploration \\nModel monitoring \\nIII Model build and train \\nModel deployment \\nI \\nII \\nIV V VI \\nModel evaluation ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6104386e-6958-4168-ae04-f50a680ad01f', embedding=None, metadata={'page_label': '3', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='      I. Data access \\nand collection \\n3/\\n3/ The first step to a machine learning problem is \\naccessing the data. Typically, data scientists will obtain the data for the business problems they are working on by querying the databases where their companies store their data. In addition, there is a lot of value in unstructured datasets that do not fit well into a relational database (e.g. logs, raw texts, images, videos, etc.). These datasets are heavily processed via Extract, Transform, Load (ETL) pipelines written by data engineers and data scientists. These datasets either reside in a data lake or in a database (either relational or not). When data scientists do not have the data needed to solve their problems, they can get the data by scraping data from websites, purchasing data from data providers or collecting the data from surveys, clickstream data, sensors, cameras, etc. \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bd84245a-9eb2-47f8-9029-67737b8851a4', embedding=None, metadata={'page_label': '4', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\n   \\n  \\n  \\n \\n \\n \\n    \\nand make decisions about any necessary data \\n4/ II. Data preparation and exploration \\nSource: Scikit Learn Library https:// \\nscikit-learn.org/, Visualization \\nperformed with Oracle Cloud Infrastructure Data Science https:// docs.cloud.oracle.com/en-us/iaas/ data-science/using/data-science.htm After getting the data, data scientists have to \\nprepare the raw data, perform data exploration, visualize data, transform data and possibly repeat the steps until it’s ready to use for modeling. Data preparation is cleansing and processing raw data before analysis. Before building any machine learning model, data scientists need to understand the available data. Raw data can be messy, duplicated or inaccurate. Data scientists explore the data available to them, then cleanse the data by identifying corrupt, inaccurate and incomplete data and replacing or deleting it. \\nIn addition, data scientists need to determine if \\nthe data has labels or not. For example, if you have a series of images and you want to develop a detection model to determine whether there is a car in the image, you need to have a set of images labeled whether there is a car in them and most likely need bounding boxes around the cars in the images. If the images lack labels, data scientists will have to label them. There are open source tools and commercial vendors that provide platforms for data labeling, as well as human labelers for hire. \\nAfter data is cleansed, data scientists explore \\nthe features (or the variables) in their dataset, identify any relationship between the features transformations. There are various tools data scientists can use for exploratory data analysis in open source libraries and analytics/data science platforms. A tool that performs statistical analysis of the dataset and creates data visualizations to generate plots of the features is useful in this step. \\nIt is important to see what types of features are \\nin the dataset. Features can be numerical, which can be a floating point or integer. Categorical features have a finite number of possible values, typically assigning data into groups. For example, if you have a dataset from a customer survey, the respondent’s gender (male or female) is a categorical feature. Ordinal features are a categorical feature with a set order or scale. For example, customer satisfaction response: very satisified, satisfied, indifferent, dissatisfied, and very dissatisfied has a set order to it. You can convert that ordering into an integer scale (1->5). After determining what kind of features there are, obtaining a distribution of values that each of the feature has and getting summary statistics of each feature would be next. Doing so would help answer the following questions about the dataset: \\n\\x97Is the dataset skewed towards a range of values \\nor a subset of categories? \\n\\x97What are the minimum, maximum, mean, me-dian and mode values of the feature? \\n\\x97Are there missing values or invalid values such as null? If so, how many are there? \\n\\x97Are there outliers in the dataset? \\nDuring the data exploration step, it is helpful to plot the features and also plot the features against each other to identify patterns in the dataset. This helps to determine the need for data transformation. Some of the questions you need to answer are: \\n\\x97How do you handle missing values? Do you \\nwant to fill in the values and if so, what ap-proach do you plan to take to fill in for the missing value? Some approaches include taking the mean value, the median, the mode, nearby entry’s value and average of nearby entries’ values. \\n\\x97How will you handle outliers? \\n\\x97Are some of your features correlated with each other? \\n\\x97Do you need to normalize the dataset or per-form some other transformation to rescale the data (e.g. log transformation)? \\n\\x97What is your approach to a long tail of categor-ical values? Do you use them as-is, group them in some meaningful way or ignore a subset of them altogether? \\nSummary statistics and visualization of features in a dataset of three types of wine and features of each wine. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='07cace43-24ce-417f-badb-7aeecb8f4b9f', embedding=None, metadata={'page_label': '5', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n     \\n   5/\\ntarget — \\nalcohol — \\nmalic_acid — \\nash — \\nalcalinity_of_ash — \\nmagnesium — \\ntotal_phenols — \\nflavanoids — \\nnonflavanoid_phenols — \\nproanthocyanins — \\ncolor_intensity — \\nhue — \\nod280/od315_of_diluted_wines — \\nproline — \\ntarget —\\nalcohol —\\nmalic_acid —\\nash —\\nalcalinity_of_ash —\\nmagnesium —\\ntotal_phenols —\\nflavanoids —\\nnonflavanoid_phenols —\\nproanthocyanins —\\ncolor_intensity —\\nhue —\\nod280/od315_of_diluted_wines —\\nproline — \\nDuring the data exploration step, you can identify \\npatterns in your dataset for ideas about how to develop new features that would better represent the dataset. This is known as feature engineering. For example, if you have a traffic dataset for the number of vehicles passing through a major intersection at every hour, you might want to create a new feature categorizing the hour into different parts of the day, such as early morning, mid-morning, early afternoon, late afternoon, and nighttime. \\nFor categorical features, often it is necessary to \\none hot encode the feature. One hot encoding means turning a categorical feature into binary features, one for each of the categories. For example, suppose you have a dataset of customers, and we have a feature on which states the customer comes from: Washington, Oregon, — 0.30 \\n— 0.25 \\n— 0.20 \\n— 0.15 \\n— 0.10 \\n— 0.05 \\n— 0.00 \\nHeatmap of how correlated the features are to each other, from a dataset with three types of wine and features of each wine. \\nand California. One hot encoding would produce \\ntwo binary features where one feature is whether a customer is from Washington state or not, and the second feature is whether a customer is from Oregon or not. It is assumed that if the customer is not from Washington or Oregon, he / she would be from California, so there is no need for a third feature. \\nSource: Scikit Learn Library https://scikit-learn.org/, \\nVisualization performed with Oracle Cloud Infrastructure \\nData Science https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm \\n5/ ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bd3be936-a9db-4374-93db-f6fdeb4e65de', embedding=None, metadata={'page_label': '6', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n    \\nIII. Model build and train \\nModel build consists of choosing the correct \\nmachine learning models to solve the problems and features that go into the models. In the first step of model build, data scientists need to decide what might be the appropriate machine learning model to solve the problem. There are two main types of machine learning models: supervised and unsupervised. Supervised learning involves modeling a set of input data to an output or a label. Classification and regression are supervised learning problems. Unsupervised learning involves modeling a set of input data without a label. For example, customer segmentation is an unsupervised learning problem. You do not know a priori what customer segment a customer belongs to. The segment will be assigned by the model. \\nDifferent classes of machine learning models \\nare used to solve unsupervised and supervised learning problems. Typically, data scientists will try multiple models and algorithms and generate multiple model candidates. Data scientists do not know a priori what model will perform best on the dataset, so they experiment with several of them. During the model training, a data scientist might do feature selection which is the process of selecting only a subset of features as input to the machine learning model. The benefits of reducing the number of input variables are to reduce computational cost of model training, make the model more generalizable and possibly improve model performance. \\nDuring the model training, the dataset is split up \\ninto training and testing sets. The training dataset is used to train the model, and the testing dataset is used to see how well the model performs on data it has not seen. Model evaluation will be discussed in more detail below. \\nModel hyperparameter tuning is a major task in \\nthe model training process. Models are algorithms, and hyperparameters are the knobs that a data scientist can tune to improve the performance of the model. For example, the depth of a decision tree is a hyperparameter. \\nYou can choose to have a very deep or very \\nshallow decision tree. This will affect the bias and variance of your model. Bias is the error from underfitting or the error from not capturing the relation between the features and the outputs. Variance is the error from overfitting where the model does well in the training dataset but does not perform well to unseen data. Tuning the hyperparameters of a model can be partially automated, although data scientists should always be involved in the process.  \\n6/ \\nData scientists also have to decide what kind of compute resources they need for training their models. You can prepare the data and train the models locally on your computer. However, depending on how much data there is to prepare and then used to train the model, your computer may not be enough. You may have to transition the workload to the cloud where you can have access to a broader selection of computing resources including GPUs. \\nSome models can be trained faster on specialized \\nhardware (e.g., training perceptrons/deep neural network models on GPUs.) You may also explore distributed training environments that can speed up the process, especially when the amount of data cannot fit in the memory of the largest machine available, through splitting and distributing the data across multiple machines, or when you want to simultaneously train multiple model candidates in parallel on separate machines. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2fdfd17c-3400-4a74-971b-f97dfa12d7fb', embedding=None, metadata={'page_label': '7', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='       7/\\nAutoML has garnered quite a bit of attention over \\nthe past few years due to the promise of being able to make machine learning more accessible to a larger audience. AutoML stands for automated machine learning. It automates the process of feature selection, model/algorithm selection, and hyperparameter tuning. It is a feature that all major data science platforms have. Users can feed a dataset to AutoML, and it will train multiple machine learning models, tune the hyperparameters for those models, and evaluate their performance against each other. \\nAutoML can improve the productivity of data \\nscientists by automating the training process. It also allows data analysts and developers to build machine learning models without tweaking every aspect of the model training process that comes with data science expertise. Most AutoML capabilities support tabular data for classification and regression problems while others have more advanced offerings that support images and text data, as well as time series forecasting. \\nThe drawback to AutoML, or any complicated \\nmodel, is it can seem like a blackbox solution making it difficult for users to understand how the models arrive at the predictions. Users should look to the model explainability offering of the AutoML system to see what capability exists to help users interpret the models and understand how the selected models arrive at the predictions. \\n7/ AutoML \\nModel explanations typically fall into global explanation and local explanation. Global explanation is understanding the general behavior of a machine learning model as a whole. This includes explaining how important each feature is in contributing to the model predictions. Local explanation provides an understanding on why the machine learning model made a particular prediction for one data sample. For example, why did a fraud detection algorithm predict a particular transaction as fraudulent? \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e6b3bfdb-77fb-4a16-a8a8-118a7b503784', embedding=None, metadata={'page_label': '8', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='      \\n      \\n  \\n    1\\n8/]1[ .0\\n1] [0 .0 0 ] [0 .0 0\\n]1[ 3 4 .0 0] 0 [2 7 .8 0 ] [2 7 8 .0 0\\n] [0 .0 0 ] [0 .0 0 ] 61[ .0\\n1IV. Model evaluation \\nThere are many open source tools that help data \\nscientists calculate the metrics for evaluating Confusion matrix for multiclass classification for the results from a random forest ROC curve for multiclass classification constructed for the results from a random \\nmodel on predicting the type of wine based on the features of a wine, from a dataset forest model on predicting the type of wine from a dataset containingmachine learning models and help them visualize containing three types of wine and features of each wine.  three types of wine and features of each wine. the metrics (e.g., AUC-ROC curve, gain and lift charts.) When evaluating machine learning models, data scientists need to decide which \\nROC curve \\nmetrics are important for the business problem \\n1.0 — \\n0.8 — \\n0.6 — \\n0.4 — \\n0.2 — \\n0.0 — True Positive Rate they are trying to solve. \\nclass_0 —For classification problems, one can use accuracy for model evaluation, but sometimes it might not be the optimal metric choice. If a problem involves detecting whether someone has a rare illness, a better metric might be how many people with the illness are accurately diagnosed divided by all the people with the illness. In that case, it would be more useful to look at a confusion matrix which shows the number of true positives, true negatives, false positives, and false negatives, and calculate precision and recall. For regression \\nTrue Labelclass_1 — \\n0.0 — \\n0.2 — \\n0.4 — \\n0  —.6\\n0  —.8\\n1.  —0\\nclass_2 — problems, you can use metrics such as root-mean- False Positive Rate \\nsquare error, mean absolute error or calculate the coefficient of determination r\\n2. For unsupervised \\nproblems, a set of clusters with high cohesion \\nclass_0 —\\nclass_1 —\\nclass_2 —within the clusters and separation between the class_0 (AUC: 1.000) class_1 (AUC: 0.990) \\nclusters is considered ideal. This can be measured \\nclass_2 (AUC: 0.990) Youden’s J Statistic with metrics such as silhouette score and Calinski- Predicted Label \\nHarabasz coefficient. \\nSource: Scikit Learn Library https://scikit-learn.org/, Visualization performed with Oracle Cloud Infrastructure Data Science https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm \\n8/ \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='781cbc91-14e4-47c6-8846-f922a2b21e07', embedding=None, metadata={'page_label': '9', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n \\n \\n \\n \\n \\n \\n    V. Model deployment \\nAfter the model training and evaluation processes \\nare complete, the best candidate models are saved. Models are usually saved in Pickle, ONNX and PMML format. Depending on the objectives, data scientists might work on a machine learning problem for proof of concept, experimentation or to deploy it to production. Model deployment is consuming the predictions made by the machine learning model in some way. Most likely, the pipeline of data transformations have to be deployed also. Typically, data scientists will work with engineers on model deployment. \\nDepending on how you intend to consume the \\npredictions, you can deploy for batch consumption or real time consumption. For batch consumption, the predictions can be scheduled (e.g., every hour, every day.) The predictions can then be stored in a database and consumed by other applications. Typically, the amount of data you process is larger than for real-time prediction. A use case would be if you run an e-commerce site and you want to send a weekly email to the customers about recommended products for them based on past purchases. The machine learning models can be scheduled to run ahead of time. For real-time consumption, a trigger would initiate the process of using the persisted model to serve a prediction. For example, deciding whether a transaction is fraudulent or not when payment is initiated, requires real-time prediction. You have to consider how quickly you have to serve the predictions (milliseconds, seconds?), the volume of demand for the service, and the size of data to run predictions on. Minimizing latency to serve prediction is important. You can improve the serving latency by using a smaller model in size, using accelerators such as GPU and improving how features related to the entity are retrieved for real-time prediction (e.g. If you are recommending products to a user as the user is browsing a site, improvements on how information on past purchases of the user is fetched can improve the latency.) \\nThere are different tools and cloud platform \\nofferings for model deployment such as Functions-as-a-Service (FaaS) platforms, fully managed deployment of models as HTTP endpoints, DIY with flask or Django in a container orchestration platform such as k8 and docker swarm, etc. \\n9/ ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c39f55a1-1f95-4a90-ba71-1d97f5b4fcb9', embedding=None, metadata={'page_label': '10', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='VI. Model monitoring \\nModel monitoring is a challenging step that is \\nsometimes forgotten by organizations without mature machine learning and data science initiatives. Model retraining and redeployment requires time from the data science and engineering team and compute resources. Model monitoring helps the team decide if and when it is necessary to retrain the model and redeploy. Model monitoring can be broken down into two components: drift/statistical monitoring of the model performance and ops monitoring. \\nAfter models are deployed, the metrics by which \\nthe models were measured and trained go down in production. This is because data is non-stationary. The non-stationarity can manifest in many ways: features in production data can take values outside of the range in the training dataset; there can be a slow drift in the distribution of the values, etc. \\nBecause of the model degradation, the quality \\nof the models need to be monitored to decide if and when to retrain the model and redeploy. Sometimes, it is not possible to immediately obtain the prediction accuracy of live data going to a production system. For example, it might take some time before you can decide whether a \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n    \\nchurn prediction model or a fraud detection model provided an accurate prediction. However, it is possible to look at the statistics and distribution of the training data compared to live data and also compare the distribution of the model predictions with training and live data. For example, if you are working with a customer churn model, you can compare the features of your customers used to train your model compared to the features of customers in the production system. Also, you can look at the percentage of customers predicted to churn in the training sample compared to the live production. \\nOps monitoring of the machine learning system will \\nrequire partnership between the data scientists and engineering team. Things to monitor include serving latency, memory/CPU usage, throughput and system reliability. Logs and metrics need to be set up for tracking and monitoring. Logs contain records of events, along with the time when they occurred. They can be used to investigate specific incidents and figure out the cause of the incident. Kibana is an open-source tool used for searching and viewing logs. Metrics measure the usage and behavior of the machine learning system. Prometheus and Grafana \\nare tools for monitoring metrics. \\n10/ ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ffdf6435-d7db-475c-a0fa-7e720cd32f53', embedding=None, metadata={'page_label': '11', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='    \\nConclusion \\nWe hope this has been a useful guide on the steps \\nit takes to build a machine learning model. It is important to remember that machine learning is a very iterative process, and the steps outlined in this book will be reiterated and improved upon many times. \\nThere are many resources available that dive \\ndeeper into each of the steps covered in this book, and you can learn more about them as you make decisions about your enterprise’s data science strategy. If you’re ready to get started, Oracle offers hands-on labs so you can experiment with building your own data science models. \\n11/ ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cf676cfd-fe65-465e-b19f-52034cfd98dd', embedding=None, metadata={'page_label': '12', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\xa0     \\n  \\n \\n \\n \\n\\u2002 \\n\\u2002 \\n               \\n \\n \\n \\n \\n     \\n \\n              Oracle corporation \\nWorldwide headquarters \\n500 Oracle Parkway, Redwood Shores, CA 94065, USA \\nWorldwide inquiries \\nTele +\\u20071.650.506.7000 +\\u20071.800.ORACLE1 \\nFax +\\u20071.650.506.7200 \\noracle.com \\nConnect with us \\nCall +1.800.ORACLE1 or visit oracle.com . Outside North America, \\nfind your local office at oracle.com/contact . \\nblogs.oracle.com/oracle \\nfacebook.com/oracle twitter.com/oracle \\nAuthor \\nWendy Yip, Data Scientist. \\nCopyright © 2020, Oracle and/or its affiliates. All rights reserved. This document is provided for information purposes only, and the contents \\nhereof are subject to change without notice. This document is not warranted to be error-free, nor subject to any other warranties or conditions, \\nwhether expressed orally or implied in law, including implied warranties and conditions of merchantability or fitness for a particular purpose. We specifically disclaim any liability with respect to this document, and no contractual obligations are formed either directly or indirectly by this document. This document may not be reproduced or transmitted in any form or by any means, electronic or mechanical, for any purpose, without our prior written permission. \\nOracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners. \\nIntel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are \\ntrademarks or registered trademarks of SPARC International, Inc. AMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks \\nor registered trademarks of Advanced Micro Devices.  UNIX is a registered trademark of The Open Group 05.10.19. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "Number of documents: 12\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "# reader = SimpleDirectoryReader(input_dir=f\".\\\\documents\\\\coe_demo\")\n",
    "\n",
    "# reader = SimpleDirectoryReader(input_files=[\"path/to/file1\", \"path/to/file2\"])\n",
    "reader = SimpleDirectoryReader(input_files=[f\".\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf\"])\n",
    "\n",
    "print(f\"Reader: {reader}\")\n",
    "documents = reader.load_data()\n",
    "print(f\"Documents: {documents}\")\n",
    "print(f\"Number of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='1802367d-178b-47db-afd0-43e8e035e678', embedding=None, metadata={'page_label': '1', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Lifecycle of  \\nmachine learning models\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Doc ID: 1802367d-178b-47db-afd0-43e8e035e678\n",
      "Text: Lifecycle of   machine learning models\n",
      "0 {'page_label': '1', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "1 Doc ID: 1f3b0ed1-ed90-4903-aef6-78209b2b086d\n",
      "Text: 1 2/Introduction   Steps of building machine learning models  2/\n",
      "Machine learning is an area that enterprises  are increasingly\n",
      "investing in or identifying as a potential area of growth. There are\n",
      "many reasons enterprises invest in machine learning, from being able\n",
      "to leverage data to find insights about their customers to making\n",
      "processes more ...\n",
      "1 {'page_label': '2', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "2 Doc ID: 7a72a0aa-7259-4373-b413-10301139d343\n",
      "Text: I. Data access  and collection  3/ 3/ The first step to a\n",
      "machine learning problem is  accessing the data. Typically, data\n",
      "scientists will obtain the data for the business problems they are\n",
      "working on by querying the databases where their companies store their\n",
      "data. In addition, there is a lot of value in unstructured datasets\n",
      "that do not fit we...\n",
      "2 {'page_label': '3', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "3 Doc ID: 74f60f76-5e36-4439-80ff-80a7c2befa9c\n",
      "Text: and make decisions about any necessary data  4/ II. Data\n",
      "preparation and exploration  Source: Scikit Learn Library https://\n",
      "scikit-learn.org/, Visualization  performed with Oracle Cloud\n",
      "Infrastructure Data Science https:// docs.cloud.oracle.com/en-us/iaas/\n",
      "data-science/using/data-science.htm After getting the data, data\n",
      "scientists have to  prep...\n",
      "3 {'page_label': '4', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "4 Doc ID: 6bcbff8d-586c-4f66-988a-268895bfd502\n",
      "Text: 5/ target —  alcohol —  malic_acid —  ash —  alcalinity_of_ash —\n",
      "magnesium —  total_phenols —  flavanoids —  nonflavanoid_phenols —\n",
      "proanthocyanins —  color_intensity —  hue —\n",
      "od280/od315_of_diluted_wines —  proline —  target — alcohol —\n",
      "malic_acid — ash — alcalinity_of_ash — magnesium — total_phenols —\n",
      "flavanoids — nonflavanoid_phenols — pro...\n",
      "4 {'page_label': '5', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "5 Doc ID: b320e428-ced2-4ca7-8148-a88dac6b7960\n",
      "Text: III. Model build and train  Model build consists of choosing the\n",
      "correct  machine learning models to solve the problems and features\n",
      "that go into the models. In the first step of model build, data\n",
      "scientists need to decide what might be the appropriate machine\n",
      "learning model to solve the problem. There are two main types of\n",
      "machine learning mode...\n",
      "5 {'page_label': '6', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "6 Doc ID: 8c55a2ba-13c3-40b1-8dd0-0f9adbf4cad5\n",
      "Text: 7/ AutoML has garnered quite a bit of attention over  the past\n",
      "few years due to the promise of being able to make machine learning\n",
      "more accessible to a larger audience. AutoML stands for automated\n",
      "machine learning. It automates the process of feature selection,\n",
      "model/algorithm selection, and hyperparameter tuning. It is a feature\n",
      "that all major ...\n",
      "6 {'page_label': '7', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "7 Doc ID: 52965357-2d10-4013-ace3-7fe02b10062a\n",
      "Text: 1 8/]1[ .0 1] [0 .0 0 ] [0 .0 0 ]1[ 3 4 .0 0] 0 [2 7 .8 0 ] [2 7\n",
      "8 .0 0 ] [0 .0 0 ] [0 .0 0 ] 61[ .0 1IV. Model evaluation  There are\n",
      "many open source tools that help data  scientists calculate the\n",
      "metrics for evaluating Confusion matrix for multiclass classification\n",
      "for the results from a random forest ROC curve for multiclass\n",
      "classification co...\n",
      "7 {'page_label': '8', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "8 Doc ID: ef1fdb5f-2398-4a9c-9935-e02ada4b47b7\n",
      "Text: V. Model deployment  After the model training and evaluation\n",
      "processes  are complete, the best candidate models are saved. Models\n",
      "are usually saved in Pickle, ONNX and PMML format. Depending on the\n",
      "objectives, data scientists might work on a machine learning problem\n",
      "for proof of concept, experimentation or to deploy it to production.\n",
      "Model deplo...\n",
      "8 {'page_label': '9', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "9 Doc ID: e945ef71-b979-4c44-af0f-5d4bf6150fe8\n",
      "Text: VI. Model monitoring  Model monitoring is a challenging step\n",
      "that is  sometimes forgotten by organizations without mature machine\n",
      "learning and data science initiatives. Model retraining and\n",
      "redeployment requires time from the data science and engineering team\n",
      "and compute resources. Model monitoring helps the team decide if and\n",
      "when it is necessa...\n",
      "9 {'page_label': '10', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "10 Doc ID: 035cb39f-8594-42a0-a0a1-ec944b27f0e0\n",
      "Text: Conclusion  We hope this has been a useful guide on the steps\n",
      "it takes to build a machine learning model. It is important to\n",
      "remember that machine learning is a very iterative process, and the\n",
      "steps outlined in this book will be reiterated and improved upon many\n",
      "times.  There are many resources available that dive  deeper into each\n",
      "of the steps...\n",
      "10 {'page_label': '11', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "11 Doc ID: 860bd344-2ad5-41d2-af73-2ab06d5cd9f1\n",
      "Text: Oracle corporation  Worldwide headquarters  500 Oracle Parkway,\n",
      "Redwood Shores, CA 94065, USA  Worldwide inquiries  Tele\n",
      "+ 1.650.506.7000 + 1.800.ORACLE1  Fax + 1.650.506.7200  oracle.com\n",
      "Connect with us  Call +1.800.ORACLE1 or visit oracle.com . Outside\n",
      "North America,  find your local office at oracle.com/contact .\n",
      "blogs.oracle.com/oracle  fa...\n",
      "11 {'page_label': '12', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "12 Doc ID: 241068ae-ba3d-4b70-a1e8-479e6849d664\n",
      "Text: Oracle Autonomous Database   Technical Overview     Available in\n",
      "the Cloud or on -premises, Oracle Autonomous  Database is self\n",
      "-driving, self -securing, and self -repairing.   June  2023  |\n",
      "Version [2.2]  Copyright © 2023 , Oracle and/or its affiliates\n",
      "Public\n",
      "12 {'page_label': '1', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "13 Doc ID: 3fbc9196-6158-400e-8afc-4786604b5932\n",
      "Text: 2 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    DISCLAIMER   This document in any form, software or printed\n",
      "matter, contains proprietary information that is the exclusive\n",
      "property  of Oracle. Your access to and use of this confidential\n",
      "material is su...\n",
      "13 {'page_label': '2', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "14 Doc ID: 63bb71c1-807f-4f8e-bb6c-670738ba449e\n",
      "Text: 3 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    TABLE OF CONTENTS   Disclaimer  2  Introduction  4\n",
      "Autonomous Database Services  5  Provisioning Databases  7  Scaling  9\n",
      "Management  10  Security  11  Data Protection  13  Optimization  15\n",
      "Autonomou...\n",
      "14 {'page_label': '3', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "15 Doc ID: a50fe9a3-ca38-4f96-a62e-80bf8130db03\n",
      "Text: 4 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public        INTRODUCTION   Oracle Autonomous Database combines the\n",
      "flexibility of cloud with the power of machine learning to deliver\n",
      "data  management as a service. It’s built upon a foundation of\n",
      "technical inn...\n",
      "15 {'page_label': '4', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "16 Doc ID: 7e0d5f8a-33f3-47f9-8c5a-1bc17eb457e1\n",
      "Text: 5 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Oracle Cloud Infrastructure   Oracle Autonomous Database\n",
      "simply would not have been possible without the Oracle Cloud and a new\n",
      "gene ration of  software -defined Infrastructure as a Service. The\n",
      "Oracle ...\n",
      "16 {'page_label': '5', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "17 Doc ID: 9ecc5137-6d19-4a23-9ea6-472e9befe4b6\n",
      "Text: 6 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Autonomous Data Warehouse   As the name implies, Oracle\n",
      "Autonomous Data Warehouse (ADW) is tailored for Data Warehouse and\n",
      "related workloads  including Data Marts, Machine Learning or as part\n",
      "of a Data ...\n",
      "17 {'page_label': '6', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "18 Doc ID: 0568abab-345a-4a8c-a94f-420949536755\n",
      "Text: 7 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Autonomous Database  on Exadata  Cloud@Customer   Some\n",
      "organizations are not able to deploy databases in the Public Cloud\n",
      "even when deployed on dedicated  infrastructure in the Public Cloud.\n",
      "For this re...\n",
      "18 {'page_label': '7', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "19 Doc ID: 600d91e8-67ce-4490-935a-badf93025f1e\n",
      "Text: 8 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Basic Provisioning Selections   Cloud Service Region\n",
      "Workload Type (Data Warehouse vs. Mixed Workload)   Display Name\n",
      "Database Name   CPU cores   Storage Space   Administrator Password\n",
      "License Typ...\n",
      "19 {'page_label': '8', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "20 Doc ID: 289cffcf-3f4a-4a80-b0c6-bbfed0299713\n",
      "Text: 9 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    By default, ATP honors all optimizer hints, while ADW is\n",
      "configured to ignore opti mizer hints. Customers can choose to  change\n",
      "this behavior by modifying the parameters OPTIMIZER_IGNORE_HINTS and /\n",
      "or ...\n",
      "20 {'page_label': '9', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "21 Doc ID: 6e957e77-a0fb-4930-b432-be1b4e6f2752\n",
      "Text: 10 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    space allocated to the database, and I/O Operations Per\n",
      "Second (IOPS) and I/O Bandwidth are effectively a function of  both\n",
      "CPU and Storage.   Controlling the nu mber of configured OCPUs is the\n",
      "primary...\n",
      "21 {'page_label': '10', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "22 Doc ID: 8d18973b-053a-4f12-b318-aea53cf66ea5\n",
      "Text: 11 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    • Oracle Virtual Machine   • Oracle Grid Infrastructure   •\n",
      "Oracle Real Application Clusters   • Oracle Database Enterprise\n",
      "Edition   All of these components are managed automatically in the\n",
      "Autonomous...\n",
      "22 {'page_label': '11', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "23 Doc ID: 0834c20c-e9ed-4f96-befd-42018cede2ed\n",
      "Text: 12 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public        All data stored in and network communication with Oracle\n",
      "Cloud are encrypted by default. Autonomous Database builds  upon this\n",
      "secure foundation through the following:   • Best Practice Security\n",
      "Co...\n",
      "23 {'page_label': '12', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "24 Doc ID: 2e8c40e2-b422-4ddf-ba04-da32c27a1354\n",
      "Text: 13 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Database User Security   Users of the Autonomous Database\n",
      "are responsible for creating database users and schema owners.\n",
      "Customers use the  ADMIN user to create and manage schema owner\n",
      "accounts which, ...\n",
      "24 {'page_label': '13', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "25 Doc ID: c4af20bd-81e6-4c49-bd16-a56271ed697a\n",
      "Text: 14 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    per day. Backup of REDO logs are also included to give point\n",
      "-in-time recovery to any time or System Change Number  (SCN) within\n",
      "the backup window.   Supplemental Database Backups   Database backups\n",
      "ar...\n",
      "25 {'page_label': '14', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "26 Doc ID: 81406876-627a-4fed-be37-aef52259716e\n",
      "Text: 15 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Recover -to-Copy   The Cloud Service Console provides a\n",
      "simple interface to restore an Autonomous Database to the po int-in-\n",
      "time of a  specific backup. The service console lists the available\n",
      "backups w...\n",
      "26 {'page_label': '15', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "27 Doc ID: aeda2c22-b395-4222-8839-91cecf1021e0\n",
      "Text: 16 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Oracle Autonomous Transaction Processing (ATP) is optimized\n",
      "for Transaction Processing and “mixed -workloads” that  include a\n",
      "mixture of Transaction Processing an d operational reporting. In ATP,\n",
      "data ...\n",
      "27 {'page_label': '16', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "28 Doc ID: 69040b45-f884-4e41-aa40-f0527dd07568\n",
      "Text: 17 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Automatic Secondary Indexing   Beyond use of indexes to\n",
      "ensure data integrity, additional indexing may be required for\n",
      "performance reasons. Primary  Key/Foreign Key relationships and unique\n",
      "keys  defin...\n",
      "28 {'page_label': '17', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "29 Doc ID: bdc7fc12-3b4e-454e-9df7-7ac96a75e7ca\n",
      "Text: 18 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Oracle SQL Developer   The Oracle SQL Developer  tool is\n",
      "widely used by Oracle database administrators and application\n",
      "developers for working  with Oracle databases and is fully compatible\n",
      "with Autonom...\n",
      "29 {'page_label': '18', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "30 Doc ID: a2e27a30-0a8a-41f1-8adb-901c6b0f3324\n",
      "Text: 19 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    SQL Plus & SQLcl   Oracle SQL Plus is frequently used by\n",
      "Database Administrators and Developers to execute ad -hoc queries,\n",
      "administer  the database, and even to build simple application\n",
      "components. SQ...\n",
      "30 {'page_label': '19', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n",
      "31 Doc ID: e50195bf-ff75-4ec0-bb5a-21bdd89032a2\n",
      "Text: 20 White Paper | Oracle Autonomous Database Technical Overview\n",
      "|  Version [2. 2]   Copyright © 2023 , Oracle and/or its affiliates |\n",
      "Public    Eliminating Legacy Data Types   Autonomous Database\n",
      "Serverless does not support legacy data types such as LONG and LOG\n",
      "RAW. These data types  were superseded by Large Objects (LOB) in\n",
      "Oracle8i, more t...\n",
      "31 {'page_label': '20', 'file_name': 'oracle-autonomous-database-technical-overview.pdf', 'file_path': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag_llamaindex\\\\documents\\\\coe_demo\\\\oracle-autonomous-database-technical-overview.pdf', 'file_type': 'application/pdf', 'file_size': 469982, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    print(i, doc)\n",
    "    print(i, doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Embeddings\n",
    "\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "# embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "# with input_typ='search_query'\n",
    "embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.getenv('COHERE_API_KEY'),\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\" # \"search_query\", \"search_document\"\n",
    ")\n",
    "\n",
    "# embeddings = embed_model.get_text_embedding(\"Hello CohereAI!\")\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Parsing nodes:   8%|▊         | 1/12 [00:00<00:05,  1.90it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:00<00:00,  7.71it/s]\n",
      "Parsing nodes:  17%|█▋        | 2/12 [00:01<00:07,  1.31it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  5.53it/s]\n",
      "Parsing nodes:  25%|██▌       | 3/12 [00:02<00:09,  1.00s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:04<00:00,  8.14it/s]\n",
      "Parsing nodes:  33%|███▎      | 4/12 [00:07<00:19,  2.47s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  8.79it/s]\n",
      "Parsing nodes:  42%|████▏     | 5/12 [00:08<00:13,  2.00s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 32/32 [00:05<00:00,  5.71it/s]\n",
      "Parsing nodes:  50%|█████     | 6/12 [00:14<00:19,  3.24s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:02<00:00,  7.38it/s]\n",
      "Parsing nodes:  58%|█████▊    | 7/12 [00:16<00:14,  2.85s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00,  7.95it/s]\n",
      "Parsing nodes:  67%|██████▋   | 8/12 [00:17<00:09,  2.47s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.13it/s]\n",
      "Parsing nodes:  75%|███████▌  | 9/12 [00:20<00:07,  2.51s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:02<00:00,  8.18it/s]\n",
      "Parsing nodes:  83%|████████▎ | 10/12 [00:23<00:05,  2.62s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  5.70it/s]\n",
      "Parsing nodes:  92%|█████████▏| 11/12 [00:24<00:02,  2.09s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:02<00:00,  7.26it/s]\n",
      "Parsing nodes: 100%|██████████| 12/12 [00:26<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "# Instead of chunking text with a fixed chunk size, the semantic splitter adaptively picks the breakpoint \n",
    "# in between sentences using embedding similarity. This ensures that a “chunk” contains sentences that are \n",
    "# semantically related to each other.\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "              buffer_size=1, \n",
    "              breakpoint_percentile_threshold=95, \n",
    "              embed_model=embed_model\n",
    "           )\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='eb08967a-c9b2-4c79-aedd-e02630a998ab', embedding=None, metadata={'page_label': '1', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a91546ca-ecbe-4fba-9d20-6688c23f60df', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'data-science-lifecycle-ebook.pdf', 'file_path': 'documents\\\\coe_demo\\\\data-science-lifecycle-ebook.pdf', 'file_type': 'application/pdf', 'file_size': 7899425, 'creation_date': '2024-05-20', 'last_modified_date': '2024-01-29'}, hash='97ce161d1443f3d1096b389c935b9e75a857ae3135d553e5a4456f29480d8d9f')}, text='Lifecycle of  \\nmachine learning models\\n', start_char_idx=0, end_char_idx=39, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/1 [31:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\llama_index\\core\\llms\\utils.py:41\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     40\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\llama_index\\llms\\openai\\utils.py:407\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mingestion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IngestionPipeline, IngestionCache\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# create the pipeline with transformations\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(\n\u001b[0;32m     10\u001b[0m     transformations\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     11\u001b[0m         SentenceSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m---> 12\u001b[0m         \u001b[43mTitleExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     13\u001b[0m         CohereEmbedding(),\n\u001b[0;32m     14\u001b[0m     ]\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(Document\u001b[38;5;241m.\u001b[39mexample())\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# run the pipeline\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\llama_index\\core\\extractors\\metadata_extractors.py:90\u001b[0m, in \u001b[0;36mTitleExtractor.__init__\u001b[1;34m(self, llm, llm_predictor, nodes, node_template, combine_template, num_workers, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nodes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes must be >= 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m---> 90\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm \u001b[38;5;129;01mor\u001b[39;00m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m,\n\u001b[0;32m     91\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     92\u001b[0m     node_template\u001b[38;5;241m=\u001b[39mnode_template,\n\u001b[0;32m     93\u001b[0m     combine_template\u001b[38;5;241m=\u001b[39mcombine_template,\n\u001b[0;32m     94\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     96\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\llama_index\\core\\settings.py:39\u001b[0m, in \u001b[0;36m_Settings.llm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[1;32mc:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\llama_index\\core\\llms\\utils.py:48\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-llms-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         )\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     59\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "# Transformations & Ingestion Pipeline\n",
    "from llama_index.core import Document\n",
    "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        CohereEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(Document.example())\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=[Document.example()])\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  53%|█████▎    | 9/17 [22:21<19:52, 149.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# The Llamaindex Settings module helps in configuring global settings that can be used throughout the pipeline.\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = Groq(model=\"llama3-70b-8192\")\n",
    "Settings.embed_model = CohereEmbedding(model_name=\"embed-english-v3.0\")\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=1024)\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 20\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=1024)]\n",
    "# maximum input size to the LLM\n",
    "Settings.context_window = 4096\n",
    "\n",
    "# number of tokens reserved for text generation.\n",
    "Settings.num_output = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Groq(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001BCD64C7280>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001BCC0232170>, completion_to_prompt=<function default_completion_to_prompt at 0x000001BCC02B6320>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='llama3-70b-8192', temperature=0.1, max_tokens=None, logprobs=None, top_logprobs=0, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='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', api_base='https://api.groq.com/openai/v1', api_version='', context_window=3900, is_chat_model=True, is_function_calling_model=True, tokenizer=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Groq(model=\"llama3-70b-8192\", api_key=os.getenv('GROQ_API_KEY'))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Groq' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me something funny\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Groq' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condarag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
